"""
Validator Model Inference

This script contains code to automate the procedure of running validator model inference on prompt variations.
It takes as input a list of bpv_idx, generate validator model output accordingly with multiple validator models, and store into corresponding Parquet file.
The score the models assign are based on predefined rubric sections.

Inputs:
- List of bpv_idx: A list of integers representing multiple prompt variations' indices.

Outputs:
- Writes validation scores to a Parquet file named `{i}_validation_score.parquet`, where `i` is the index
  of the base prompt.
- The output structure consists of **2n + 1 columns**, with n rubric sections. The last column is the total score, weighted.
- The first n columns contain a **tuple of k values**, corresponding to scores assigned by the `k` number of validator models.
Example format of a single file, if n = 3 and k = 3:
| bpv_idx | section_1 | section_2 | section_3 | section_1_avg | section_2_avg | section_3_avg | total_score |
|---------|-----------|-----------|-----------|---------------|---------------|---------------|-------------|
| (1, 0)  | (2, 1, 3) | (1, 2, 3) | (3, 1, 2) | 2.0           | 2.0           | 2.0           | 4.0         |
| (1, 1)  | (3, 2, 1) | (2, 1, 3) | (1, 3, 2) | 2.0           | 2.0           | 2.0           | 4.0         |
| ...     | ...       | ...       | ...       | ...           | ...           | ...           | ...         |

Process:
1. Reads the bp_idx as a script parameter
2. Collects the base prompt strings for the given bp_idx
3. Collects the main model output strings for the given bp_idx
4. Collects the model input text file and the configuration file
5. Performs inference on all validator models using the provided main model outputs and model instructions
6. Opens or creates the corresponding Parquet file and writes results to it.

Dependencies:
? `main_model_output`: The output generated by the main model.
- `val_model_input.txt`: A text file containing instructions for validation.
- `validator_model_config.yaml`: A YAML configuration file specifying parameters for the validator models.
"""

from src.utils import prompt, data_handler
from src.utils.prompt import BasePrompt, MainModelOutput, ValidationScore
from src.utils.data_handler import ValidationScoreParquet
from configs.root_paths import *
import yaml
import json
import os

from transformers import AutoModelForCausalLM, AutoTokenizer
from mistral_inference.transformer import Transformer
from mistral_inference.generate import generate

from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
from mistral_common.protocol.instruct.messages import UserMessage
from mistral_common.protocol.instruct.request import ChatCompletionRequest

import torch

# Step 2: Collects the base prompt strings for the given bp_idx
def collect_base_prompt_str(bp_idx):
    """Collects the base prompt string for the given bp_idx."""
    base_prompt = BasePrompt(bp_idx)
    prompt_str = base_prompt.get_prompt_str()
    return prompt_str

def load_model(model_name):
    """Loads the model and tokenizer."""

    if model_name in(['falcon-mamba', 'opt']):
      device = "cuda" if torch.cuda.is_available() else "cpu"
      if device != "cuda":
          raise RuntimeError("CUDA is not available. Please check your setup.")
      tokenizer = AutoTokenizer.from_pretrained()
      model = AutoModelForCausalLM.from_pretrained("gpt2")
      return model, tokenizer

    elif model_name in(['mistral']):
      device = "cuda" if torch.cuda.is_available() else "cpu"
      if device != "cuda":
          raise RuntimeError("CUDA is not available. Please check your setup.")
      tokenizer = MistralTokenizer.from_file("./mistral-nemo-instruct-v0.1/tekken.json")  # change to extracted tokenizer file
      model = Transformer.from_folder("./mistral-nemo-instruct-v0.1")  # change to extracted model dir
      
    else:
      raise ValueError("Model name not found")
      
    return model, tokenizer

def construct_model_input(base_prompt_str, main_model_output):
    """Constructs the model input by replacing placeholders in the val_model_input.txt template"""
    input_file_path = os.path.join(MODEL_INPUT_PATH, 'val_model_input.txt')
    rubric_file_path = os.path.join(MODEL_INPUT_PATH, 'rubric.txt')

    # Read the input file
    with open(input_file_path, 'r') as file:
        model_input = json.load(file)

    # Read the rubric file
    with open(rubric_file_path, 'r') as file:
        rubric_content = file.read()

    # Replace placeholders with actual values
    model_input['content_template'] = model_input['content_template'].replace("{base_prompt_str}", base_prompt_str)
    model_input['content_template'] = model_input['content_template'].replace("{main_model_output}", main_model_output)
    model_input['content_template'] = model_input['content_template'].replace("{rubric_text}", rubric_content)

    return model_input

# CONFIRM EVERYTHING BELOW

def model_config():
    """Loads the model configuration file."""
    config_path = VALIDATOR_MODEL_CONFIGS

    with open(config_path, 'r') as file:
      configs = yaml.safe_load(file)

    return configs

def validator_model_inference_per_prompt_variation(pv_obj, model_name):
    """
    Performs validator model inference using the provided prompt variation.

    Inputs:
    -pv_obj (PromptVariation): The prompt variation object.

    Returns:
    - str: The model output string.
    """
    base_prompt_str = pv_obj.get_base_prompt_str()
    main_model_output = pv_obj.get_main_model_output()
    prompt = construct_model_input(base_prompt_str, main_model_output)
    model, tokenizer = load_model(model_name)

    configs = model_config()

    # reading all relevant configs
    temperature = configs.get("temperature")
    top_p = configs.get("top_p")
    top_k = configs.get("top_k")

    if model_name in(['falcon-mamba', 'opt']):

      inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

      with torch.no_grad():
          output = model.generate(
              **inputs,
              temperature=temperature,
              top_p=top_p,
              top_k=top_k,
          )

      return tokenizer.decode(output[0], skip_special_tokens=True)

    if model_name in(['mistral']):
        completion_request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])

        inputs = tokenizer.encode_chat_completion(completion_request).inputs

        out_tokens, _ = generate([inputs],
                                 model,
                                 temperature=temperature,
                                 top_p=top_p,
                                 top_k=top_k
                                 eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id) # not sure what this is for, might be able to be removed
        return tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])


# def validator_model_inference_per_base_prompt(bp_idx):
#     # Less priority, used in testing pipeline afaik
#     '''
#     Performs main model inference on all prompt variations for the given bp_idx. Stores all the outputs in its respective Parquet file.

#     Input:
#     - bp_idx (int): The base prompt index.
#     '''
#     all_pv_outputs = []
#     mo_parquet = prompt.ModelOutputParquet(bp_idx)
#     for pv_obj in collect_prompt_variations(bp_idx):
#         model_output = validator_model_inference_per_prompt_variation(pv_obj)
#         all_pv_outputs.append((pv_obj.get_prompt_index(), model_output))
#     mo_parquet.insert_model_outputs(all_pv_outputs)


# will call script on list of base prompts
# needs main function to do so

def main(bpv_idx):
    # Collect base prompt strings and main model inputs
    base_prompt_str = collect_base_prompt_str(bpv_idx)
    main_model_output = MainModelOutput.model_output_str(bpv_idx)

    # Prepare the model input
    model_input = construct_model_input(base_prompt_str, main_model_output)

    # Load the model and tokenizer
    model_names = ['falcon-mamba', 'opt', 'mistral']  # this could be implemented better/in a different file but its here for now
    models_tokenizers = {model_name: load_model(model_name) for model_name in model_names}

    # Perform inference for each model and collect outputs
    all_outputs = []
    for model_name, (model, tokenizer) in models_tokenizers.items():
        output_text = validator_model_inference_per_prompt_variation(model_input, model_name)
        all_outputs.append((model_name, output_text))
  
    # Store and write the outputs to the parquet file
    validation_scores = ValidationScore(ValidationScoreParquet(bpv_idx))
    for model_name, output_text in all_outputs:
        validation_scores.parse_and_store_scores(output_text)
    
    scores_data = validation_scores.save_validation_scores()
    validation_scores.vs_parquet.save_scores_to_parquet(scores_data)

    print(f"Validation scores for bpv_idx {bpv_idx} have been saved to the Parquet file.")

if __name__ == "__main__":
    bpv_idx = [0]  # Replace with actual list of bpv_idx
    main(bpv_idx)